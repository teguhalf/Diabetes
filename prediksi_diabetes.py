# -*- coding: utf-8 -*-
"""Prediksi_Diabetes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/150aSlptjB_E1tYDyE2tfLQpXXksLj7kz

# **1. IMPORT LIBRARY**
"""

import pandas as pd # Manipulasi Data
import numpy as np # Komputasi Matematika
from sklearn.model_selection import train_test_split # Membagi dataset (train, val, test)
from sklearn.preprocessing import QuantileTransformer # Standardisasi
from sklearn.metrics import classification_report, confusion_matrix # Metrik Akurasi
import matplotlib.pyplot as plt # Visualisasi Data
import seaborn as sns # Visualisasi Data
import tensorflow as tf # Library Neural Network
from tensorflow.keras.models import Sequential # Input Layer
from tensorflow.keras.layers import Dense, Dropout # Fully Connected Layer dan Dropout Layer
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # Callbacks

"""# **2. LOAD DATASET**"""

df = pd.read_csv("diabetes.csv") # Membuat dataframe dengan nama df
df.head() # Menampilkan 5 baris teratas dataframe df

"""# **3. EXPLORATORY DATA ANALYSIS**"""

# Mengidentifikasi informasi dataset
print('INFORMASI DATASET:')
print(df.info())

# Mengindentifikasi nilai statistik tiap kolom melalui deskripsi statistik
print('DESKRIPSI STATISTIK DATASET:')
print(df.describe())

# Mengindentifikasi data yang null
print('JUMLAH DATA NULL:')
print(df.isnull().sum())

# Mengindentifikasi data duplikat
print('JUMLAH DATA DUPLIKAT:')
print(df.duplicated().sum())

# Visualisasi Distribusi Tiap Kolom
df.hist(bins=50, figsize=(10, 8))
plt.tight_layout()
plt.show()

# Membuat fungsi untuk membagi tiga rentang usia (Remaja, Dewasa, Tua)
def categorize_age(age):
  if 0 < age < 18:
    return 'Remaja'
  elif 18 <= age < 40:
    return 'Dewasa'
  elif age >= 40:
    return 'Tua'

# Membuat kolom Age_Group
df['Age_Group'] = df['Age'].apply(categorize_age)

# Visualisasi Pie Chart Blood Pressure Category
age_group_counts = df['Age_Group'].value_counts()
plt.figure(figsize=(4, 4))
plt.pie(age_group_counts, labels=age_group_counts.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightgreen', 'salmon'])
plt.title('Distribusi Rentang Usia')
plt.show()

print('\nJumlah Data per Kelompok Usia:')
print(df['Age_Group'].value_counts())

# Membuat fungsi untuk membagi tiga tipe Blood Pressure (Rendah, Normal, Tinggi)
def categorize_blood_pressure(bp):
  if bp < 80:
    return 'Rendah'
  elif 80 <= bp < 120:
    return 'Normal'
  elif bp >= 120:
    return 'Tinggi'

# Membuat kolom BloodPressure_Category
df['BloodPressure_Category'] = df['BloodPressure'].apply(categorize_blood_pressure)

# Visualisasi Pie Chart Blood Pressure Category
blood_pressure_counts = df['BloodPressure_Category'].value_counts()
plt.figure(figsize=(4, 4))
plt.pie(blood_pressure_counts, labels=blood_pressure_counts.index, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightgreen', 'salmon'])
plt.title('Distribusi Kategori Tekanan Darah')
plt.show()

print('\nJumlah Data per Kategori Tekanan Darah:')
print(df['BloodPressure_Category'].value_counts())

"""# **4. CLEANING DATA**"""

# Ubah nilai 0 pada bbrp kolom dengan nilai rata-rata masing-masing kolom
columns_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in columns_to_replace:
    df[col] = df[col].replace(0, np.nan)
    df[col].fillna(df[col].mean(), inplace=True)

df.drop('Age_Group', axis=1, inplace=True)
df.drop('BloodPressure_Category', axis=1, inplace=True)

# Membuat variabel cols berisikan semua kolom, kecuali Outcome
cols = df.columns.tolist()
cols.remove('Outcome')

# Identifikasi Outlier
def identify_outlier_iqr(data):
    outliers = []
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr # batas bawah
    upper_bound = q3 + 1.5 * iqr # batas atas
    for i in data:
        if i < lower_bound or i > upper_bound:
            outliers.append(i)
    return outliers

print('Sebelum Impute Outliers')
data_outlier = {} # Dictionary utk menyimpan nilai yg dianggap outlier
for col in cols:
    data_outlier[col] = identify_outlier_iqr(df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

# Fungsi untuk menangani outlier
def impute_outlier_iqr(df):
    Q1 = df.quantile(0.25) # Quartil 1
    Q3 = df.quantile(0.75) # Quartil 2
    IQR = Q3-Q1 # Interquartil
    lower_bound = Q1 - 1.5*IQR # Batas Bawah (BB)
    upper_bound = Q3 + 1.5*IQR # Batas Atas (BA)
    # Mengganti nilai outlier yg melebihi BA, dengan nilai BA
    df = np.where(df > upper_bound, upper_bound, df)
    # Mengganti nilai outlier yg kurang dari BB, dengan nilai BB
    df = np.where(df < lower_bound, lower_bound, df)
    return df

# Menerapkan fungsi ke semua kolom
for col in cols:
    df[col] = impute_outlier_iqr(df[col])

print('Setelah Drop Outliers')
data_outlier = {}
for col in cols:
    data_outlier[col] = identify_outlier_iqr(df[col])
    print('Outlier (' + col + '):', len(data_outlier[col]), 'outliers')

# Menentukan variabel X dan Y
X = df.drop('Outcome', axis=1) # Kolom selain 'Outcome'
y = df['Outcome'] # Hanya kolom 'Outcome'

# Quantile Transformer
scaler = QuantileTransformer()
X_scaled = scaler.fit_transform(X)

# Membagi dataset ke data train, data val, dan data test
X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)

!pip install keras-tuner

!rm -rf tuning_diabetes

import keras_tuner as kt
from tensorflow.keras.optimizers import Adam

# Membangun fungsi untuk percobaan mencari jumlah layer terbaik
def build_model(hp):
    model = Sequential()
    # Input Layer
    model.add(Dense(
        units=hp.Int('units_input', min_value=16, max_value=64, step=16),
        activation='relu',
        input_shape=(X_train.shape[1],)))
    # Layer Dropout 1
    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.3, step=0.1)))
    # Hidden Layer
    model.add(Dense(
        units=hp.Int('units_hidden', min_value=8, max_value=16, step=8),
        activation='relu'))
    # Layer Dropout 2
    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.3, step=0.1)))
    # Output Layer
    model.add(Dense(1, activation='sigmoid'))

    # Menggunakan optimizer Adam
    optimizer_choice = hp.Choice('optimizer', values=['adam'])
    if optimizer_choice == 'adam':
        opt = Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-4]))

    model.compile(
        optimizer=opt,
        loss='binary_crossentropy',
        metrics=['accuracy']) # Metrik Akurasi

    return model

# Setup tuning
tuner = kt.RandomSearch(
    build_model, # Masukkan fungsi mencari jumlah layer terbaik
    objective='val_accuracy', # Akurasi pada data validation
    max_trials=10, # Jumlah Percobaan Tuning
    executions_per_trial=1, # Eksekusi Tiap Percobaan
    directory='tuning_diabetes', # Direktori File Hasil Tuning
    project_name='diabetes_model' # Nama File Tuning
)

# Menjalankan pencarian parameter
tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)

# Mendapatkan model terbaik
best_model = tuner.get_best_models(num_models=1)[0]
best_hps = tuner.get_best_hyperparameters(1)[0]

print("\nBest Hyperparameters:")
for param in best_hps.values:
    print(f"{param}: {best_hps.values[param]}")

# Buat model menggunakan dari hyperparameter tuning
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# Kompilasi model
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Ringkasan arsitektur
model.summary()

# Menerapkan callbacks untuk penghentian awal otomatis
EarlyStopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

# Callbacks untuk menyimpan model yang dianggap terbaik
checkpoint = ModelCheckpoint(
    'best_model_diabetes.h5',
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# Melakukan iterasi pada model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=5,
    callbacks=[EarlyStopping, checkpoint],
    verbose=1
)

loss, accuracy = model.evaluate(X_test, y_test)
print(f'\nAkurasi pada data uji: {accuracy:.2f}')

# Visualisasi Accuracy pada data training dan data validation
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Akurasi Selama Pelatihan')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()

# Visualisasi Loss pada data training dan data validation
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Selama Pelatihan')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()

# Melakukan prediksi dengan data test
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Melihat nilai akurasi dengan Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from tensorflow.keras.models import load_model
best_model = load_model('best_model_diabetes.h5')

# Prepare new data for inference (replace with your actual new data)
# Assuming new_data is a pandas DataFrame with the same columns as the training data, excluding 'Outcome'
new_data = pd.DataFrame({
    'Pregnancies': [0, 3, 5, 2, 10],
    'Glucose': [85, 140, 180, 130, 190],
    'BloodPressure': [66, 80, 74, 72, 130],
    'SkinThickness': [29, 35, 0, 25, 15],
    'Insulin': [0, 130, 200, 90, 100],
    'BMI': [26.6, 35.0, 45.3, 30.1, 20.1],
    'DiabetesPedigreeFunction': [0.351, 0.672, 1.201, 0.134, 0.841],
    'Age': [25, 40, 50, 33, 39]
})

# Scale the new data using the same scaler fitted on the training data
new_data_scaled = scaler.transform(new_data)

# Make predictions
predictions = best_model.predict(new_data_scaled)

# Convert probabilities to binary predictions (0 or 1)
predicted_classes = (predictions > 0.5).astype(int)

print("\nInference Results:")
for i in range(len(new_data)):
  print(f"Data Point {i+1}:")
  print(f"  Original Data: {new_data.iloc[i].to_dict()}")
  print(f"  Predicted Probability of Diabetes: {predictions[i][0]:.4f}")
  print(f"  Predicted Class (0: No Diabetes, 1: Diabetes): {predicted_classes[i][0]}")